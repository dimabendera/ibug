Influence Estimation
---
- Line up numbers in Tables 2 and 3
	* 2-level sort: metric, then alphabetical.
- Consider adding batch size for TracInCP to make it more consistent with the original paper.
- Consider expanding the explanation for Feldman and Zhang before introducing Subsample.
	* Should the name SubSample be changed? Why is the second "s" capitalized?
- Run paper through grammarly.
- Fix capitalization errors in references.
- Fix typo in the abstract.
- Change indicator function to be a subscript of the 1 (and remove brackets?) in eq. (14).
- Add Daniel and Zayd influence paper reference into the paper.

- Add mention of comparison to LeafInfluence (abstract). DONE.
- Maybe provide some detail about the empirical measures and how rankings
	may change depending on the measure (introduction). DONE.
- Make sure sheer number of methods compared comes through (introduction). DONE.
- Remove Z* from early introduction. DONE.
- De-emphasize qualitative analysis from DL papers in introduction. DONE.
- Un-hyphenate "deep-learning". DONE.
- Change order of empirical findings; our main finding is NOT that
	LeafInfluence is a poor approximation of LOO (introduction). DONE.
- DShap is not "more principled", it gives an average LOO given more context (Sec. 2.1). DONE.
	* Could do "Another approach..."; or something more subtle. DONE.
- Check capitalization of "Representer-Point" and "Influence Functions". DONE.
- Merge last par. of sec. 3.1 to prev. par. DONE.
- Mention LeafInfluence-SP before BoostIn, fully defining or providing a preview
	in the LeafInfluence section (Sec. 3.1). DONE.
- Combine last sentence of Sec. 3.2. to previous paragraph (Sec. 3.2). DONE.
- Make new paragraph in after equations in Sec. 3.2.1 and merge last paragraph
	to the previous one (Sec. 3.2.1). DONE.
- Two examples are TREATED identically, sec. 3.3. DONE.
- Fix "regresion" (Sec. 3.3, footnote 6). DONE.
- How to interpret Figure 5, make text clearer about the significance. DONE.
- Potentially summarize experiment descriptions before Figure 2; then supplementary
	analysis comes after. Still mention extra analysis for each sec. in appendix (experiments). DONE.
	* Keep backup. DONE.
	* Move runtime comparison to earlier in supplementary analysis. DONE.


Uncertainty GBRTs
---

- Overlap formula: m1 + r1 * x = m2 + r2 * x; solve for x.
	* m1/m2: train time.
	* r1/r2: predictions/sec.
	* x = (m2 - m1) / (r1 - r2)
- Kin8nm NLL performance for the 5 folds, p-value=0.069 even though KGBM
	always wins! Is this a fair representation of KGBM?
  KGBM  PGBM
-0.747 -0.705
-0.778 -0.699
-0.682 -0.341
-0.751 -0.471
-0.715 -0.671
- Figures 1 and 2 should say "Test NLL" instead of just "NLL". DONE.
- Add each paragraph for NGBoost and PGBM in Sec. 3.1.
- Add paragraph at beginning of Sec. 4 to contrast KGBM with other methods.
- Illustrative figure for affinity computation.
- Wilcoxon signed rank test for numbers in Table 1. hopefully p < 0.01.
- Explain temporary rho and our heuristic for setting rho on validation data.
- Include value of k in Figure 2, probably next to the dataset.
- Create Algs. 1 and 2. DONE.
- Update prob. regression literature repo.
- Updates Tables 1, 2, 3, 4 and Figure 2.
- Try to summarize Table 3.
- Add table breaking down total runtime into: train and test.
	* Add third column showing when PGBM and KGBM amortized time would intersect.
- Make sure validation ranking matches test ranking for Table 2 and Figure 2.
- Paired t-test or bold everything whose s.e. overlaps with the best method. DONE.
	* Binomial sign test of 17-5 is sig. p = 0.0169.
- Add dummy column between models in Table 3 for extra separation. DONE.
	* Added vertical lines between methods.
- Wine using multinomial for Figure 2. Keep as is for now. DONE.
	* Actually NO! Because you could use classification instead.
	* Could do MEPS, MSD, and Wine; but for now, keep it as is.
- Make summarization plots for tree_frac and dist.
- Try to send Daniel a draft by Sunday.

- KDD formatting:
	* Sections should be numbered.
	* Captions should go above tables.
	* Captions should go below figures.
- Background.
- KGBM.
- Results to show:
	* Main results (NLL & RMSE). TENTATIVE.
	* Auxiliary results:
		+ Build time comparisons and predict time comparisons.
		+ Additional GBRT base model results.
		+ Posterior distribution modeling. TENTATIVE.
		+ Subsampling efficiency/accuracy tradeoff.
- Related work.
	* Predicting confidence intervals using error residuals (conformal predictions). (traditional)
		+ Typically assumes a normality distribution of the error residuals.
		+ Taieb et al.
	* Bayesian Methods. (traditional)
		+ BART, Bayesian DL models.
		+ Requires costly sampling operations such as MCMC.
	* Predict quantiles. (related, but significantly different)
		+ More focused on probability and ranking than prediction uncertainty.
		+ Produces a relative ranking of training examples and new predictions.
		+ QRF (RF), LSF (agnostic), DL Approaches.
		+ Quantiles are useful on their own.
		+ Translating quantiles to parametric distributions is NOT straightforward.
	* Directly model parameters of a parametric distribution.
		+ GAMLSS (generalized additive models), XGB/LGBM/CB-LSS and NGBoost (GBMs).
	* PGBMs
		+ Treat leaf values as stochastic random variables (RVs).
		+ Add RVs together from leaves the test example traverses.
- Conclusion.

- Change improve point prediction to competitively. DONE.

- Read LSF paper. DONE.
	* LSF groups examples based on only the OUTPUT of the model.
	* This does not group by example similarity/affinity:
		+ See reviewer eTYD - 2) https://openreview.net/forum?id=VD3TMzyxKK
- Read Forecasting w/ Trees paper. DONE.
	* Written by LSF authors, gives context into forecasting with trees.
- Read quantile regression forests (QRFs) paper. DONE.
	* Special case of LSF for RF models.
	* Similar to KGBM, but different.
		+ Theory of QRF is specifically designed for RFs.
		+ They use the estimated conditional mean (avg. weighted response over
                 	all train examples) as the central prediction.
		+ Each training example gets a weight: 1 / no. examples at its leaf,
			summed over all trees in the forest.
		+ A weighted distribution over ALL training examples is used to
			model the distribution of the estimated conditional mean.
	* To directly apply QRFs to boosted regression trees would require:
		+ Modeling the location as a weighted sum of the training examples.
			This is not how GBRTs are predicted! GBRTs are ADDITIVE models.
		+ Modeling the scale as a weighted distribution of ALL training examples.
			This is MUCH more costly than using the k-nearest examples.
			This would also require computing the weight of ALL training
			examples (visiting EVERY leaf) for EVERY new prediction!
		+ The QRFs algorithm recomputes every leaf weight for x_i for each
			new prediction x. The leaf weights for x_i can be precomputed;
			however, the weight of x w.r.t. x_i still needs to be computed.
			Thus, the runtime complexity for computing	 w_i(x) is O(T), and
			O(Tn) for all train examples in which n = no. train, and T = no. trees.
	* KGBM, in contrast, uses the k-nearest neighbors based on an UNWEIGHTED affinity.
		+ The distribution is also UNWEIGHTED.
		+ For location, the ORIGINAL prediction is used instead of a weighted average
			response from ALL training examples.
		+ KGBM has a worst-case runtime complexity of O(nT) but an avg.
			case of O(mT) in which m ~= n / no. leaves (assuming balanced leaves).
                          Computing weights for ALL examples would SEVERELY limit scalability.
	* QRFs are doing QUANTILE REGRESSION, i.e., in what quantile does the output of my
		test prediction fall w.r.t. my entire training set.
	* NGBoost/PGBM/KGBM only provide the data (or aleatoric) uncertainty for a given prediction.
		+ Maybe quantiles can be computed from the predicted distribution, whether
			that be parametric or non-parametric?
- Read conformal predictions paper. DONE.
- Read about M5 prediction dataset. DONE.
- Read NGBoost paper. DONE.
	* NGBoost model estimates the parameters of a distribution.
	* Distributional parameters must be chosen beforehand.
		+ An Ensemble of trees is built for each parameter.
- Read XGBoostLSS paper. DONE.
	* XBoostLSS and CatBoostLSS models estimate the parameters of a distribution.
- Read PGBM paper. DONE.
	* The model is NOT a wrapper, but still optimizes MSE.
	* Computes location and scale for each prediction using the stochasticity of leaves.
		+ Any distribution that uses location and scale can be used.
		+ Theoretically, a different distribution could be used for each prediction,
			but in practice, it is more likely to optimize a distribution
			across all examples.
	* Experiments use max_leaves=16 and max_bin=64 likely to speed up computation.
		+ May be really slow for a larger number of leaves and/or max. no. bins.
	* Implemented in PyTorch and thus can optimize arbitrary loss functions.
- Read M5 accuracy competition: Results, findings, and conclusions.

- Motivation of KGBM:
	* More interpretable predictions (point estimate + uncertainty).
	* Forecasting models are increasingly popular for high-stakes applications
		such as medical, weather, m5 predictions.
	* Trees outperform DL (Kaggle, M5 prediction, used in industry).
	* Libraries are "battle-hardened".
		+ Can be used as black-box learners, allowing users to
			focus on feature engineering, hyper parameter tuning, etc.
		+ Fast to train.
		+ Less investment than trying to select a DL architecture,
		 	which is more of an art.

- Confidence Interval.
	* Uses empirical samples to quantity the uncertainty of a statistic like the population mean.
		+ E.g., estimating the mean of a regression output response, its true value.
- Prediction Interval.
	* Estimates the uncertainty of an individual data point.
		+ E.g., Estimating the uncertainty of a model prediction, not its true value.
	* Is always wider than the confidence interval.


V6

- Changes made
	* Added 301, 401, 501, 601, and 701 to KNN k search values.
	* Small change to the creation of "leaf_dict" in KGBM.
		+ Assigns array of train examples to EACH leaf, not just
			ones assigned from the apply method.
		+ This means some leafs can have empty arrays (typically CatBoost).
		+ Fixes key errors if a test example falls into a leaf
			that no training examples have been in.
		+ Only affects building KGBM runtime not predicting.
		+ Should not affect runtime complexity significantly.
	* Added support for XGB and CB for "tree_type".


V5

- Changes made
	* Added "grid_search" arg (constant, KGBM, PGBM).
		+ Set grid search values to be consistent across models.
	* Added "tree_frac" arg (KGBM) for faster predictions.
	* PGBM is now working on Talapas!
	* Getting sample statistics straight from the prediction (PGBM) instead of
		approximating them from 1,000 forecasts.
	* Set "min_scale_pct" default to 0 (deactivate) for KNN.
		+ Tuning "delta" should help with the catastrophic failures.
	* Added 301, 401, 501, 601, and 701 to list of k values to search (KGBM).
	* Changed the following dataset names:
		+ "ames_housing" -> "ames"
		+ "cal_housing" -> "california"
		+ "fb" -> "facebook"
		+ "online_news" -> "news"
		+ "synth_regression" -> "synthetic"

- Potential changes
	* Increase speed, options:
		+ Use sparse vectors for affinities (check sparseness of affinity vectors).
		+ Randomly sample a fraction of trees to approx. exact affinity vector.
		+ Save leaf weights and only sample from leaves with a small no. examples.
	* Get PGBM working on Talapas.
	* Use fixed hyper parameters for LGB as PGBM and just tune no. iter.
	* Add XGBoostLSS, LightGBMLSS, or CatboostLSS as an additional baseline.
	* Add LSF as an additional baseline.
	* Add conformal predictions as an additional baseline.
	* Add datasets: bike, communities, superconductor, star.


V4

- Results
	* Using min. scale for KNN makes KNN an INEFFECTIVE uncertainty estimator.
	* The min. scale method for KGBM is very EFFECTIVE.
	* Even w/ delta, KGBM is generally outperforming NGBoost and PGBM.
		+ Constant w/ delta is much more effective than constant w/o delta.
		+ KGBM w/ delta is not any more effective than KGBM w/o delta,
			except for the Concrete dataset.
		+ Delta somewhat helps out NGBoost and PGBM, but not much.

- Changes made
	* Use min. scale value when predicting to prevent catastrophic failure.
		+ KGBM: Set min. scale value to be the min. scale value on validation data.
		+ KNN: Set min. scale value to be the 0.1 (default) percentile of sorted (asc.)
			scale values computed on validation data.
	* Delta was missing 1e1 in values to search through.
		+ Also added 1e-8, 1e-7, and 1e3 to list of delta values.

- Potential changes
	* Set scale values < min. s.d. Min. s.d. could be: value at 0.1 percentile
		of (sorted) squared errors on validation data (0.1% of predictions
                 in expectation will have this exact same minimum uncertainty prediction).
                 This percentage can be a hyper-parameter for KGBM.
	* Add more datasets.
	* Remove datasets: online_news, synth_regression.
	* KGBM: CV tuning.


V3

- Results
	* Catastrophic failures can happen when the k-nearest neighbors all have
		the same or very similar values (see setting a min. s.d. above).
	* KGBM is better than the other methods on average. NGBoost is better
		on very small (<1,000 samples) datasets.
	* PGBM slightly better than NGBoost.
	* Constant w/ delta can be better than everything sometimes!
	* MSD is missing for PGBM, and KGBM and NGBoost delta variants.
	* KNN is fastest, followed by constant; KGBM and NGBoost are about even;
		PGBM is slightly faster than KGBM and NGBoost.

- Changes made
	* Added delta variants.
	* Train on train+val data.
	* Create 5 non-overlapping folds per dataset.
	* NGBoost: find n_iter on val. data (max. 2k), then train on train+val data.
	* PGBM: find n_iter on val. data (max. 2k), then train on train+val data.
		+ Fix hyperparameters to be the same as in the original paper.
	* Constant and KGBM's LGB, and KNN are tuned using gridsearch.
	* Changed synth_regression to "Friedman1" problem from SKLearn.
	* Removed heart dataset.
	* Delta tuned via grid search on validation data using values
		[1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0, 1e0, 1e1, 1e2],
		and operation tuned using values ['add', 'mult'].


V2

- Results
	* KGBM is best on avg. for CRPS, NLL, and RMSE.; effect sizes are small but
		 the significant advantage is pretty consistent.
	* In terms of time compared with NGBoost, it is a wash overall;
		KGBM is sometimes orders of magnitude faster, slower, or tied.
	* NGBoost does better on SMALL datasets (e.g., <1,000 examples).
	* Synthetic regression dataset is too easy; no distinction between methods.
	* Online news dataset is too hard; no distinction between methods.
	* KNN approaches suffer when the targets for the k-nearest neighbors of a prediction are
		all the same -> zero variance. This can result in a NaN NLL or CRPS if an unseen
		example has a different target value. Mitigations:
		+ Add a very small amount of variance to the scale value (e.g., 1e-5 up to 1e-15).
		+ Add bias scale term to all or extremely confident scale predictions.
		+ Tune k more rigorously (i.e., using cross-validation or a larger val. size).
		+ Increase k until std. dev. > eps (e.g., 1e-15); do not use during tuning
			since this could lead to very small values of k.
		+ Some combination of the above.
